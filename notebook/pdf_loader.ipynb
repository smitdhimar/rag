{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791e4209",
   "metadata": {},
   "source": [
    "Load all pdfs with the document and directory load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0684d291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91875\\Documents\\projects\\rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader , DirectoryLoader\n",
    "\n",
    "def load_directory(directory_path:str):\n",
    "    dir_loader = DirectoryLoader(\n",
    "        directory_path,\n",
    "        glob=\"**/*.pdf\",\n",
    "        loader_cls= PyPDFLoader,\n",
    "        show_progress=False\n",
    "    )\n",
    "    \n",
    "    directory_documents = dir_loader.load()\n",
    "    return directory_documents\n",
    "\n",
    "directory_documents = load_directory(\"../data/pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ea5fa",
   "metadata": {},
   "source": [
    "Add splitting logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f88bc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 19 documents into 99 chunks\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'iText 2.1.7 by 1T3XT; modified using iText® 5.2.1 ©2000-2012 1T3XT BVBA', 'creator': 'JasperReports Library version 5.6.0', 'creationdate': '2025-09-01T15:59:15+05:30', 'moddate': '2025-09-01T15:59:15+05:30', 'source': '..\\\\data\\\\pdf\\\\Policy_schedule_doc - 2025-09-01T155919.522.pdf', 'total_pages': 9, 'page': 0, 'page_label': '1'}, page_content='Welcome to Bajaj Allianz Family\\n BARDOLI(VSO)-Shop Nos.25 To 28, Mezzanine Floor, , Megh Mayur Plazaratan\\nBaug, , Surat Dumas Roadsurat, Surat, Gujarat, INDIA, 395007, 9999999999\\nPolicy issuing office and correspondence address for communication by\\npolicyholder for claim, service request, notice, summons, etc.:\\n   Insured Name Pragneshkumar Rajubhai Nayka  Policy Number  12-1805-0007923327-00\\nName: Pragneshkumar Rajubhai Nayka\\nAddress:\\nLine 1: Parsigali Faliyu , Boriya T Mahuva D Surat')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "\n",
    "def split_documents(directory_documents, chunk_size = 1000 , chunk_overlap = 200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators= [\"\\n\\n\", \"\\n\"]\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(directory_documents)\n",
    "    print(f\"Split {len(directory_documents)} documents into {len(split_docs)} chunks\")\n",
    "    return split_docs\n",
    "\n",
    "\n",
    "chunks = split_documents(directory_documents, 500, 100)\n",
    "chunks[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8f432",
   "metadata": {},
   "source": [
    "chunking done. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf36e1",
   "metadata": {},
   "source": [
    "embeddings \n",
    "convert chunks to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97cfda34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import uuid\n",
    "from typing import List, Dict, Any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a3210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings \n",
    "# class structure\n",
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str= \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully, dimension of embeddings: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading the model {self.model_name}: {e}\");\n",
    "            raise\n",
    "    def generate_embeddings(self, texts: List[str])-> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded yet\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar= True)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0bace48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Store\n",
    "class VectorStore:\n",
    "    # init method ( self , name of the collection in which the vectors are to be stored, persistent location of directory where the vector store is intended to be stored)\n",
    "    def __init__(self, collection_name: str = 'vectors-store' , persistent_directory: str = '../vector_store'):\n",
    "        self.collection_name = collection_name\n",
    "        self.persistent_directory = persistent_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persistent_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path = self.persistent_directory)\n",
    "\n",
    "            # get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name = self.collection_name,\n",
    "                metadata={\n",
    "                    \"description\": \"PDF document embeddings for RAG\",\n",
    "                    \"hnsw:space\": \"cosine\"\n",
    "                }\n",
    "\n",
    "            )\n",
    "            print(\"Vector store initialized\")\n",
    "            print(f\"existing documents in collection: {self.collection.count()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing the vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[any], embeddings: np.ndarray):\n",
    "        # data for chroma db \n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        document_text = []\n",
    "        embedding_list = []\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i \n",
    "            metadata[\"content_length\"] = len(doc.page_content)\n",
    "\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            document_text.append(doc.page_content)\n",
    "            embedding_list.append(embedding.tolist())\n",
    "            \n",
    "        # add to collection \n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids= ids,\n",
    "                embeddings= embedding_list,\n",
    "                metadatas= metadatas,\n",
    "                documents= document_text\n",
    "            )\n",
    "            print(\"documents added\")\n",
    "        except Exception as e:\n",
    "            print(\"There some error \",e )\n",
    "            raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2364ad92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully, dimension of embeddings: 384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:04<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "# get text from chunks \n",
    "texts = [ doc.page_content for doc in chunks]\n",
    "embedding_manager = EmbeddingManager()\n",
    "generated_embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa1c85ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized\n",
      "existing documents in collection: 153\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# store in the vector database\n",
    "\n",
    "vector_store = VectorStore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1c3afa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents added\n"
     ]
    }
   ],
   "source": [
    "vector_store.add_documents(chunks, generated_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "42c70a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag retrieval pipeline\n",
    "class RAGretriever:\n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrivel(self, query: str, top_k: int = 5, score_threshold: float= 0.0) -> List[Dict[str, Any]]:\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings= [query_embedding.tolist()],\n",
    "                n_results= top_k\n",
    "            )\n",
    "            retrieved_documents = []\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids= results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # print(f\"i: {i} \\ndoc_id: {doc_id} \\nmetadata: {metadata} \\ndocument: {document}\\ndistance: {distance}\")\n",
    "                    similarity_score = 1 - distance\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_documents.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank':i+1\n",
    "                        })\n",
    "                # for i in retrieved_documents:\n",
    "                #     print(i['content'])\n",
    "                print(f\"retrieved {len(retrieved_documents)} results\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "\n",
    "            return retrieved_documents;\n",
    "        except Exception as e: \n",
    "            print(f\"Erorr: {e}\")\n",
    "            return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d58fb82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved 3 results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rag_retriever = RAGretriever(vector_store, embedding_manager)\n",
    "result = rag_retriever.retrivel(\"\"\" PARSIGALI FALIYU , BORIYA T MAHUVA D SURAT  ,   ALLU SURAT 395620 GUJARAT\n",
    "\"\"\", 3, 0.3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2454fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare llm class for sending messages\n",
    "import requests\n",
    "class OllamaLLM:\n",
    "    def __init__(self, model_name:str=\"gemma3:1b\", host_name:str=\"http://localhost:11434/api/generate\", is_stream:bool=False):\n",
    "        self.model_name = model_name\n",
    "        self.host_name = host_name\n",
    "        self.is_stream = is_stream\n",
    "\n",
    "    def send_message(self, retriver_object: RAGretriever, question: str):\n",
    "        try:\n",
    "            contexts = rag_retriever.retrivel(question, 3, 0.3);\n",
    "            payload = {\n",
    "                \"model\": self.model_name,\n",
    "                \"prompt\": f\"\"\"Given the context and the user question return the answer of the user question by referring context, if enough details are not found in context try to elaborate in your own way \\n context: { \"\\n\".join(i[\"content\"] for i in contexts) }\n",
    " \\n\n",
    "                question: {question} \\n\"\"\" ,\n",
    "                \"stream\": self.is_stream\n",
    "            }\n",
    "\n",
    "            response = requests.post(self.host_name, json=payload)\n",
    "            if(response.status_code == 200):\n",
    "                data = response.json();\n",
    "                return data\n",
    "            else:\n",
    "                print(f\"There is some error in api : {response}\")\n",
    "                return {}\n",
    "        except Exception as e:\n",
    "            print(f\"There is some error : {e}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5c4a99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved 3 results\n",
      "The context doesn’t state who the holder of the insurance policy is. It only focuses on “group companies or any other person in connection with the Insurance Policy or otherwise.”  It mentions safeguarding personal information, but doesn’t identify the individual or entity responsible for that safeguarding. \n",
      "\n",
      "**To answer your question directly, the context doesn't provide information about the holder of the insurance policy.**\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "llm = OllamaLLM()\n",
    "response = llm.send_message(rag_retriever,'who is the holder of the insurance policy?')\n",
    "print(response['response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f386879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
